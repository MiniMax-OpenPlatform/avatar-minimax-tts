#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
TTSæ•°å­—äººç”Ÿæˆç³»ç»Ÿ
ç”¨æˆ·è¾“å…¥æ–‡æœ¬å’ŒTTSå‚æ•°ï¼Œè‡ªåŠ¨ç”Ÿæˆè¯­éŸ³å¹¶åˆ¶ä½œæ•°å­—äººè§†é¢‘
"""

import argparse
import gc
import json
import os
import requests
import subprocess
import threading
import time
import traceback
import uuid
from enum import Enum
import queue
import shutil
from functools import partial
from tempfile import NamedTemporaryFile

import cv2
import gradio as gr
from flask import Flask, request

import service.trans_dh_service
from h_utils.custom import CustomError
from y_utils.config import GlobalConfig
from y_utils.logger import logger
from simple_motion_controller import SimpleMotionController, SimpleMotionConfig

os.environ["GRADIO_SERVER_NAME"] = "0.0.0.0"


class TTSProvider:
    """TTSæœåŠ¡æä¾›å•†"""
    MINIMAX = "Minimax"


def write_video_gradio(
    output_imgs_queue,
    temp_dir,
    result_dir,
    work_id,
    audio_path,
    result_queue,
    width,
    height,
    fps,
    watermark_switch=0,
    digital_auth=0,
    temp_queue=None,
):
    """è‡ªå®šä¹‰è§†é¢‘å†™å…¥å‡½æ•°"""
    output_mp4 = os.path.join(temp_dir, "{}-t.mp4".format(work_id))
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    result_path = os.path.join(result_dir, "{}-r.mp4".format(work_id))
    video_write = cv2.VideoWriter(output_mp4, fourcc, fps, (width, height))
    print("Custom VideoWriter init done")
    try:
        while True:
            state, reason, value_ = output_imgs_queue.get()
            if type(state) == bool and state == True:
                logger.info(
                    "Custom VideoWriter [{}]è§†é¢‘å¸§é˜Ÿåˆ—å¤„ç†æ­£å¸¸ç»“æŸ".format(work_id)
                )
                video_write.release()
                break
            elif type(state) == bool and state == False:
                logger.error(
                    "Custom VideoWriter [{}]è§†é¢‘å¸§é˜Ÿåˆ—å¤„ç†å¼‚å¸¸ç»“æŸï¼Œå¼‚å¸¸åŸå› :[{}]".format(
                        work_id, reason
                    )
                )
                video_write.release()
                result_queue.put(
                    [
                        False,
                        "[{}]è§†é¢‘å¸§é˜Ÿåˆ—å¤„ç†å¼‚å¸¸ç»“æŸï¼Œå¼‚å¸¸åŸå› :[{}]".format(work_id, reason),
                    ]
                )
                return
            else:
                # logger.info('Custom VideoWriter[{}] write img_index[{}]'.format(work_id, value_))
                # åŸå§‹app.pyä½¿ç”¨çš„æ˜¯for result_img in value_:ï¼Œæˆ‘ä»¬éœ€è¦ä¿æŒä¸€è‡´
                for result_img in value_:
                    video_write.write(result_img)

        logger.info("Custom VideoWriterå¼€å§‹åå¤„ç†")
        # ä½¿ç”¨ffmpegè¿›è¡Œè§†é¢‘ç¼–ç å¹¶æ·»åŠ éŸ³é¢‘
        if os.path.exists(audio_path):
            command = "ffmpeg -loglevel warning -y -i {} -i {} -c:a aac -c:v libx264 -crf 15 -strict -2 {}".format(
                audio_path, output_mp4, result_path
            )
            logger.info("command:{}".format(command))
        else:
            command = "ffmpeg -loglevel warning -y -i {} -i {} -c:a aac -c:v libx264 -crf 15 -strict -2 {}".format(
                audio_path, output_mp4, result_path
            )
            logger.info("Custom command:{}".format(command))
        subprocess.call(command, shell=True)
        print("###### Custom Video Writer write over")
        print(f"###### Video result saved in {os.path.realpath(result_path)}")
        result_queue.put([True, result_path])
    except Exception as e:
        logger.error(
            "Custom VideoWriter [{}]è§†é¢‘å¸§é˜Ÿåˆ—å¤„ç†å¼‚å¸¸ç»“æŸï¼Œå¼‚å¸¸åŸå› :[{}]".format(
                work_id, e.__str__()
            )
        )
        result_queue.put(
            [
                False,
                "[{}]è§†é¢‘å¸§é˜Ÿåˆ—å¤„ç†å¼‚å¸¸ç»“æŸï¼Œå¼‚å¸¸åŸå› :[{}]".format(
                    work_id, e.__str__()
                ),
            ]
        )
    logger.info("Custom VideoWriter åå¤„ç†è¿›ç¨‹ç»“æŸ")


# é‡å†™æœåŠ¡çš„write_videoå‡½æ•°
service.trans_dh_service.write_video = write_video_gradio


class TTSService:
    """TTSè¯­éŸ³åˆæˆæœåŠ¡"""

    def __init__(self):
        self.minimax_url = "https://api.minimax.chat/v1/t2a_v2"
        self.supported_models = [
            "speech-01",
            "speech-01-hd",
            "speech-02",
            "speech-02-hd"
        ]
        self.voice_options = {
            "male-qn-qingse": "é’æ¶©é’å¹´ç”·å£°",
            "male-qn-jingying": "ç²¾è‹±ç”·å£°",
            "male-qn-badao": "éœ¸é“ç”·å£°",
            "male-qn-daxuesheng": "å¤§å­¦ç”Ÿç”·å£°",
            "female-qn-qingse": "é’æ¶©é’å¹´å¥³å£°",
            "female-qn-jingying": "ç²¾è‹±å¥³å£°",
            "female-qn-badao": "éœ¸é“å¥³å£°",
            "female-qn-daxuesheng": "å¤§å­¦ç”Ÿå¥³å£°",
            "female-shaonv": "å°‘å¥³éŸ³è‰²",
            "female-yujie": "å¾¡å§éŸ³è‰²",
            "female-chengshu": "æˆç†Ÿå¥³æ€§éŸ³è‰²",
            "female-tianmei": "ç”œç¾å¥³æ€§éŸ³è‰²",
        }

    def generate_audio(self, api_key, voice_id, text, model="speech-02-hd"):
        """
        ä½¿ç”¨Minimax APIç”ŸæˆéŸ³é¢‘

        Args:
            api_key: Minimax APIå¯†é’¥
            voice_id: å£°éŸ³ID
            text: è¦åˆæˆçš„æ–‡æœ¬
            model: ä½¿ç”¨çš„æ¨¡å‹

        Returns:
            str: ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        try:
            # éªŒè¯å‚æ•°
            if not api_key or api_key == 'api_key':
                raise ValueError("è¯·æä¾›æœ‰æ•ˆçš„API Key")

            if not text.strip():
                raise ValueError("è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬")

            # è®°å½•ç”¨æˆ·è¾“å…¥çš„æ¨¡å‹å’Œå£°éŸ³IDï¼ˆå…è®¸è‡ªå®šä¹‰è¾“å…¥ï¼‰
            logger.info(f"ä½¿ç”¨æ¨¡å‹: {model}, å£°éŸ³ID: {voice_id}")

            # å‡†å¤‡è¯·æ±‚æ•°æ®
            payload = json.dumps({
                "model": model,
                "text": text.strip(),
                "voice_setting": {
                    "voice_id": voice_id,
                }
            })

            headers = {
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            }

            logger.info(f"å¼€å§‹TTSåˆæˆ: æ¨¡å‹={model}, å£°éŸ³={voice_id}, æ–‡æœ¬é•¿åº¦={len(text)}")

            # å‘é€è¯·æ±‚
            response = requests.post(
                self.minimax_url,
                headers=headers,
                data=payload,
                timeout=30
            )

            if response.status_code != 200:
                raise Exception(f"TTS APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")

            # è§£æå“åº”
            parsed_json = json.loads(response.text)

            if 'data' not in parsed_json or 'audio' not in parsed_json['data']:
                raise Exception(f"TTS APIå“åº”æ ¼å¼é”™è¯¯: {response.text}")

            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            audio_data = bytes.fromhex(parsed_json['data']['audio'])

            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
            temp_audio = NamedTemporaryFile(delete=False, suffix='.mp3', prefix='tts_')
            temp_audio.write(audio_data)
            temp_audio.close()

            logger.info(f"TTSåˆæˆæˆåŠŸ: éŸ³é¢‘æ–‡ä»¶ä¿å­˜åˆ° {temp_audio.name}")
            logger.info(f"Trace-Id: {response.headers.get('Trace-Id', 'N/A')}")

            return temp_audio.name

        except Exception as e:
            logger.error(f"TTSåˆæˆå¤±è´¥: {str(e)}")
            raise


class TTSDigitalHumanProcessor:
    """TTSæ•°å­—äººå¤„ç†å™¨"""

    def __init__(self):
        self.task = service.trans_dh_service.TransDhTask()
        self.basedir = GlobalConfig.instance().result_dir
        self.tts_service = TTSService()
        self.is_initialized = False
        self._initialize_service()
        print("TTSDigitalHumanProcessor init done")

    def _initialize_service(self):
        """åˆå§‹åŒ–æ•°å­—äººæœåŠ¡"""
        logger.info("åˆå§‹åŒ–TTSæ•°å­—äººæœåŠ¡...")
        try:
            time.sleep(5)
            logger.info("TTSæ•°å­—äººæœåŠ¡åˆå§‹åŒ–å®Œæˆã€‚")
            self.is_initialized = True
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–TTSæ•°å­—äººæœåŠ¡å¤±è´¥: {e}")

    def generate_digital_human_from_video(
        self,
        audio_input_mode,
        api_key,
        voice_id,
        text,
        model,
        audio_file,
        video_file,
        motion_mode="æ— åŠ¨ä½œå¢å¼º",
        motion_intensity=1.0,
        still_weight=0.5,
        nod_weight=0.3,
        tilt_weight=0.2,
        interval_min=2.0,
        interval_max=5.0,
        nod_amplitude_min=3.0,
        nod_amplitude_max=8.0,
        tilt_amplitude_min=3.0,
        tilt_amplitude_max=8.0
    ):
        """
        ä»æ–‡æœ¬æˆ–éŸ³é¢‘æ–‡ä»¶ç”Ÿæˆæ•°å­—äººè§†é¢‘

        Args:
            audio_input_mode: éŸ³é¢‘è¾“å…¥æ¨¡å¼ ('tts' æˆ– 'upload')
            api_key: Minimax APIå¯†é’¥ (TTSæ¨¡å¼å¿…éœ€)
            voice_id: å£°éŸ³ID (TTSæ¨¡å¼å¿…éœ€)
            text: è¦åˆæˆçš„æ–‡æœ¬ (TTSæ¨¡å¼å¿…éœ€)
            model: TTSæ¨¡å‹ (TTSæ¨¡å¼å¿…éœ€)
            audio_file: éŸ³é¢‘æ–‡ä»¶ (uploadæ¨¡å¼å¿…éœ€)
            video_file: è§†é¢‘æ–‡ä»¶
            motion_mode: åŠ¨ä½œæ¨¡å¼
            motion_intensity: åŠ¨ä½œå¼ºåº¦
            still_weight: é™æ­¢çŠ¶æ€æƒé‡
            nod_weight: ç‚¹å¤´åŠ¨ä½œæƒé‡
            tilt_weight: å€¾æ–œåŠ¨ä½œæƒé‡
            interval_min: æœ€å°åŠ¨ä½œé—´éš”æ—¶é—´
            interval_max: æœ€å¤§åŠ¨ä½œé—´éš”æ—¶é—´
            nod_amplitude_min: ç‚¹å¤´æœ€å°å¹…åº¦
            nod_amplitude_max: ç‚¹å¤´æœ€å¤§å¹…åº¦
            tilt_amplitude_min: å€¾æ–œæœ€å°å¹…åº¦
            tilt_amplitude_max: å€¾æ–œæœ€å¤§å¹…åº¦

        Returns:
            tuple: (è§†é¢‘è·¯å¾„, éŸ³é¢‘åˆ†ææŠ¥å‘Š, åŠ¨ä½œåˆ†ææŠ¥å‘Š)
        """
        while not self.is_initialized:
            logger.info("æœåŠ¡å°šæœªå®Œæˆåˆå§‹åŒ–ï¼Œç­‰å¾… 1 ç§’...")
            time.sleep(1)

        work_id = str(uuid.uuid1())
        code = work_id
        temp_audio_path = None
        audio_analysis = ""

        try:
            # æ ¹æ®è¾“å…¥æ¨¡å¼å¤„ç†éŸ³é¢‘
            if audio_input_mode == "tts":
                # TTSæ¨¡å¼ï¼šç”ŸæˆéŸ³é¢‘
                logger.info("å¼€å§‹TTSè¯­éŸ³åˆæˆ...")
                temp_audio_path = self.tts_service.generate_audio(api_key, voice_id, text, model)
                audio_analysis = self._generate_tts_analysis(api_key, voice_id, text, model, temp_audio_path)
            elif audio_input_mode == "upload":
                # ä¸Šä¼ æ¨¡å¼ï¼šä½¿ç”¨ç”¨æˆ·ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶
                if audio_file is None:
                    raise gr.Error("è¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶")
                temp_audio_path = audio_file
                audio_analysis = self._generate_upload_analysis(audio_file)
                logger.info(f"ä½¿ç”¨ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶: {audio_file}")
            else:
                raise gr.Error("æ— æ•ˆçš„éŸ³é¢‘è¾“å…¥æ¨¡å¼")

            # å¤„ç†è§†é¢‘
            cap = cv2.VideoCapture(video_file)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            cap.release()

            # åŠ¨ä½œæ§åˆ¶å¤„ç†
            motion_analysis = self._apply_motion_control(
                motion_mode, motion_intensity, work_id,
                still_weight, nod_weight, tilt_weight,
                interval_min, interval_max,
                nod_amplitude_min, nod_amplitude_max,
                tilt_amplitude_min, tilt_amplitude_max
            )

            # ç”Ÿæˆæ•°å­—äººè§†é¢‘
            logger.info("å¼€å§‹ç”Ÿæˆæ•°å­—äººè§†é¢‘...")
            self.task.task_dic[code] = ""
            self.task.work(temp_audio_path, video_file, code, 0, 0, 0, 0)

            result_path = self.task.task_dic[code][2]
            final_result_dir = os.path.join("result", code)
            os.makedirs(final_result_dir, exist_ok=True)
            os.system(f"mv {result_path} {final_result_dir}")
            os.system(
                f"rm -rf {os.path.join(os.path.dirname(result_path), code + '*.*')}"
            )
            result_path = os.path.realpath(
                os.path.join(final_result_dir, os.path.basename(result_path))
            )

            logger.info(f"æ•°å­—äººè§†é¢‘ç”Ÿæˆå®Œæˆ: {result_path}")
            return result_path, audio_analysis, motion_analysis

        except Exception as e:
            logger.error(f"TTSæ•°å­—äººç”Ÿæˆå¤±è´¥: {e}")
            raise gr.Error(str(e))
        finally:
            # æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶(åªåˆ é™¤TTSç”Ÿæˆçš„æ–‡ä»¶ï¼Œä¸åˆ é™¤ç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶)
            if audio_input_mode == "tts" and temp_audio_path and os.path.exists(temp_audio_path):
                try:
                    os.unlink(temp_audio_path)
                except:
                    pass

    def _generate_tts_analysis(self, api_key, voice_id, text, model, audio_path):
        """ç”ŸæˆTTSåˆ†ææŠ¥å‘Š"""
        import time
        current_time = time.strftime("%Y-%m-%d %H:%M:%S")

        # è·å–éŸ³é¢‘æ–‡ä»¶ä¿¡æ¯
        audio_size = 0
        if os.path.exists(audio_path):
            audio_size = os.path.getsize(audio_path)

        voice_name = self.tts_service.voice_options.get(voice_id, voice_id)

        analysis_lines = [
            f"ğŸ¤ TTSè¯­éŸ³åˆæˆæŠ¥å‘Š",
            f"ğŸ• ç”Ÿæˆæ—¶é—´: {current_time}",
            "",
            f"ğŸ“ è¾“å…¥æ–‡æœ¬: {text[:100]}{'...' if len(text) > 100 else ''}",
            f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦",
            "",
            f"ğŸ”§ TTSå‚æ•°:",
            f"  ğŸ”‘ API Provider: Minimax",
            f"  ğŸ¯ æ¨¡å‹: {model}",
            f"  ğŸµ å£°éŸ³: {voice_name} ({voice_id})",
            "",
            f"ğŸ“Š éŸ³é¢‘è¾“å‡º:",
            f"  ğŸ“ æ–‡ä»¶å¤§å°: {audio_size / 1024:.1f} KB",
            f"  ğŸ¶ æ ¼å¼: MP3",
            f"  â±ï¸  é¢„ä¼°æ—¶é•¿: {audio_size / 4000:.1f} ç§’",
            "",
            f"âœ… TTSåˆæˆçŠ¶æ€: æˆåŠŸ",
            "",
            "ğŸ” è´¨é‡æŒ‡æ ‡:",
            "  - è¯­éŸ³æ¸…æ™°åº¦: é«˜æ¸…å“è´¨",
            "  - è¯­è°ƒè‡ªç„¶åº¦: AIä¼˜åŒ–",
            "  - æƒ…æ„Ÿè¡¨è¾¾: æ™ºèƒ½è¯†åˆ«",
            "",
            "âš ï¸  æ³¨æ„: APIè°ƒç”¨æ¶ˆè€—tokensï¼Œè¯·åˆç†ä½¿ç”¨"
        ]

        return "\n".join(analysis_lines)

    def _generate_upload_analysis(self, audio_file_path):
        """ç”Ÿæˆä¸Šä¼ éŸ³é¢‘æ–‡ä»¶çš„åˆ†ææŠ¥å‘Š"""
        import time
        import os
        current_time = time.strftime("%Y-%m-%d %H:%M:%S")

        # è·å–éŸ³é¢‘æ–‡ä»¶ä¿¡æ¯
        audio_size = 0
        file_name = "æœªçŸ¥"
        if audio_file_path and os.path.exists(audio_file_path):
            audio_size = os.path.getsize(audio_file_path)
            file_name = os.path.basename(audio_file_path)

        analysis_lines = [
            f"ğŸµ éŸ³é¢‘æ–‡ä»¶ä¸Šä¼ æŠ¥å‘Š",
            f"ğŸ• å¤„ç†æ—¶é—´: {current_time}",
            "",
            f"ğŸ“ æ–‡ä»¶ä¿¡æ¯:",
            f"  ğŸ“‹ æ–‡ä»¶å: {file_name}",
            f"  ğŸ“ æ–‡ä»¶å¤§å°: {audio_size / 1024:.1f} KB",
            f"  ğŸ“ æ–‡ä»¶è·¯å¾„: {audio_file_path}",
            "",
            f"ğŸ”§ å¤„ç†æ–¹å¼:",
            f"  ğŸ¤ éŸ³é¢‘æ¥æº: ç”¨æˆ·ä¸Šä¼ ",
            f"  ğŸ¶ æ ¼å¼æ”¯æŒ: WAV, MP3, M4A, FLAC, OGG",
            f"  âš¡ å¤„ç†æ¨¡å¼: ç›´æ¥ä½¿ç”¨",
            "",
            f"âœ… éŸ³é¢‘æ–‡ä»¶çŠ¶æ€: å·²æ¥æ”¶",
            "",
            f"ğŸ“Š å¤„ç†ä¼˜åŠ¿:",
            "  - æ— éœ€APIè°ƒç”¨ï¼ŒèŠ‚çœæˆæœ¬",
            "  - æ”¯æŒè‡ªå®šä¹‰éŸ³é¢‘å†…å®¹",
            "  - ä¿æŒåŸå§‹éŸ³è´¨",
            "  - å¤„ç†é€Ÿåº¦æ›´å¿«",
            "",
            "âš ï¸  æ³¨æ„: è¯·ç¡®ä¿éŸ³é¢‘æ–‡ä»¶è´¨é‡è‰¯å¥½ï¼Œä»¥è·å¾—æœ€ä½³è§†é¢‘æ•ˆæœ"
        ]

        return "\n".join(analysis_lines)

    def _apply_motion_control(self, motion_mode, motion_intensity, work_id,
                             still_weight=0.5, nod_weight=0.3, tilt_weight=0.2,
                             interval_min=2.0, interval_max=5.0,
                             nod_amplitude_min=3.0, nod_amplitude_max=8.0,
                             tilt_amplitude_min=3.0, tilt_amplitude_max=8.0):
        """åº”ç”¨éšæœºåŠ¨ä½œæ§åˆ¶å¹¶ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        import time
        current_time = time.strftime("%Y-%m-%d %H:%M:%S")

        motion_analysis_lines = [
            f"ğŸ“Š éšæœºåŠ¨ä½œåˆ†ææŠ¥å‘Š - ä»»åŠ¡ID: {work_id}",
            f"ğŸ• å¤„ç†æ—¶é—´: {current_time}",
            "",
            f"ğŸ­ é€‰æ‹©çš„åŠ¨ä½œæ¨¡å¼: {motion_mode}",
            f"âš¡ åŠ¨ä½œå¼ºåº¦: {motion_intensity:.1f}",
            ""
        ]

        # åŸºäºé€‰æ‹©çš„åŠ¨ä½œæ¨¡å¼ç”Ÿæˆéšæœºæ§åˆ¶å‚æ•°
        if motion_mode == "æ— åŠ¨ä½œå¢å¼º":
            motion_analysis_lines.extend([
                "âœ… ä½¿ç”¨é»˜è®¤é¢éƒ¨è¡¨æƒ…æ§åˆ¶",
                "ğŸ“ˆ éšæœºåŠ¨ä½œ: å…³é—­",
                "ğŸ¯ é¢„æœŸæ•ˆæœ: è‡ªç„¶çš„å”‡å‹åŒæ­¥"
            ])
        elif motion_mode == "è½»å¾®ç‚¹å¤´":
            # ç”Ÿæˆè½»å¾®ç‚¹å¤´çš„éšæœºæ—¶é—´çº¿
            config = SimpleMotionConfig(
                switch_interval_range=(3.0, 6.0),
                motion_weights={'still': 0.6, 'nod': 0.4, 'tilt': 0.0},
                nod_range=(3.0 * motion_intensity, 6.0 * motion_intensity)
            )
            controller = SimpleMotionController(config)
            timeline = controller.generate_motion_timeline(8.0)

            motion_analysis_lines.extend([
                "âœ… å¯ç”¨è½»å¾®éšæœºç‚¹å¤´åŠ¨ä½œ",
                f"ğŸ“Š ç‚¹å¤´è§’åº¦èŒƒå›´: Â±{3.0 * motion_intensity:.1f}Â° ~ Â±{6.0 * motion_intensity:.1f}Â°",
                f"â±ï¸  éšæœºåˆ‡æ¢é—´éš”: 3-6ç§’",
                f"ğŸ¬ ç”ŸæˆåŠ¨ä½œæ®µæ•°: {len(timeline)}",
                "ğŸ¯ é¢„æœŸæ•ˆæœ: è‡ªç„¶çš„éšæœºç‚¹å¤´ç¡®è®¤"
            ])

        elif motion_mode == "æ˜æ˜¾ç‚¹å¤´":
            config = SimpleMotionConfig(
                switch_interval_range=(2.0, 4.0),
                motion_weights={'still': 0.3, 'nod': 0.7, 'tilt': 0.0},
                nod_range=(6.0 * motion_intensity, 12.0 * motion_intensity)
            )
            controller = SimpleMotionController(config)
            timeline = controller.generate_motion_timeline(8.0)

            motion_analysis_lines.extend([
                "âœ… å¯ç”¨æ˜æ˜¾éšæœºç‚¹å¤´åŠ¨ä½œ",
                f"ğŸ“Š ç‚¹å¤´è§’åº¦èŒƒå›´: Â±{6.0 * motion_intensity:.1f}Â° ~ Â±{12.0 * motion_intensity:.1f}Â°",
                f"â±ï¸  éšæœºåˆ‡æ¢é—´éš”: 2-4ç§’",
                f"ğŸ¬ ç”ŸæˆåŠ¨ä½œæ®µæ•°: {len(timeline)}",
                "ğŸ¯ é¢„æœŸæ•ˆæœ: æ¸…æ™°çš„éšæœºç‚¹å¤´æ‰‹åŠ¿"
            ])

        elif motion_mode == "æ€è€ƒæ­ªå¤´":
            config = SimpleMotionConfig(
                switch_interval_range=(4.0, 8.0),
                motion_weights={'still': 0.4, 'nod': 0.1, 'tilt': 0.5},
                tilt_range=(5.0 * motion_intensity, 15.0 * motion_intensity)
            )
            controller = SimpleMotionController(config)
            timeline = controller.generate_motion_timeline(8.0)

            motion_analysis_lines.extend([
                "âœ… å¯ç”¨æ€è€ƒæ€§éšæœºæ­ªå¤´åŠ¨ä½œ",
                f"ğŸ“Š å€¾æ–œè§’åº¦èŒƒå›´: Â±{5.0 * motion_intensity:.1f}Â° ~ Â±{15.0 * motion_intensity:.1f}Â°",
                f"â±ï¸  éšæœºåˆ‡æ¢é—´éš”: 4-8ç§’",
                f"ğŸ¬ ç”ŸæˆåŠ¨ä½œæ®µæ•°: {len(timeline)}",
                "ğŸ¯ é¢„æœŸæ•ˆæœ: æ€è€ƒçŠ¶æ€çš„éšæœºå¤´éƒ¨å€¾æ–œ"
            ])

        elif motion_mode == "éšæœºæ··åˆ":
            config = SimpleMotionConfig(
                switch_interval_range=(2.0, 5.0),
                motion_weights={'still': 0.4, 'nod': 0.35, 'tilt': 0.25},
                nod_range=(4.0 * motion_intensity, 8.0 * motion_intensity),
                tilt_range=(3.0 * motion_intensity, 10.0 * motion_intensity)
            )
            controller = SimpleMotionController(config)
            timeline = controller.generate_motion_timeline(8.0)

            motion_analysis_lines.extend([
                "âœ… å¯ç”¨å®Œå…¨éšæœºæ··åˆåŠ¨ä½œ",
                f"ğŸ“Š åŠ¨ä½œç»„åˆ: ç‚¹å¤´+å€¾æ–œ+é™æ­¢",
                f"â±ï¸  éšæœºåˆ‡æ¢é—´éš”: 2-5ç§’",
                f"ğŸ¬ ç”ŸæˆåŠ¨ä½œæ®µæ•°: {len(timeline)}",
                "ğŸ¯ é¢„æœŸæ•ˆæœ: è‡ªç„¶çš„éšæœºå¤´éƒ¨åŠ¨ä½œç»„åˆ"
            ])

        elif motion_mode == "è‡ªå®šä¹‰é…ç½®":
            # è§„èŒƒåŒ–æƒé‡
            total_weight = still_weight + nod_weight + tilt_weight
            if total_weight > 0:
                normalized_weights = {
                    'still': still_weight / total_weight,
                    'nod': nod_weight / total_weight,
                    'tilt': tilt_weight / total_weight
                }
            else:
                normalized_weights = {'still': 1.0, 'nod': 0.0, 'tilt': 0.0}

            config = SimpleMotionConfig(
                switch_interval_range=(interval_min, interval_max),
                motion_weights=normalized_weights,
                nod_range=(nod_amplitude_min * motion_intensity, nod_amplitude_max * motion_intensity),
                tilt_range=(tilt_amplitude_min * motion_intensity, tilt_amplitude_max * motion_intensity)
            )
            controller = SimpleMotionController(config)
            timeline = controller.generate_motion_timeline(8.0)

            motion_analysis_lines.extend([
                "âœ… å¯ç”¨è‡ªå®šä¹‰éšæœºåŠ¨ä½œé…ç½®",
                f"ğŸ“Š åŠ¨ä½œæƒé‡: é™æ­¢({normalized_weights['still']:.2f}) ç‚¹å¤´({normalized_weights['nod']:.2f}) å€¾æ–œ({normalized_weights['tilt']:.2f})",
                f"â±ï¸  éšæœºåˆ‡æ¢é—´éš”: {interval_min:.1f}-{interval_max:.1f}ç§’",
                f"ğŸ“ ç‚¹å¤´å¹…åº¦: Â±{nod_amplitude_min * motion_intensity:.1f}Â° ~ Â±{nod_amplitude_max * motion_intensity:.1f}Â°",
                f"ğŸ“ å€¾æ–œå¹…åº¦: Â±{tilt_amplitude_min * motion_intensity:.1f}Â° ~ Â±{tilt_amplitude_max * motion_intensity:.1f}Â°",
                f"ğŸ¬ ç”ŸæˆåŠ¨ä½œæ®µæ•°: {len(timeline)}",
                "ğŸ¯ é¢„æœŸæ•ˆæœ: å®Œå…¨è‡ªå®šä¹‰çš„éšæœºåŠ¨ä½œç»„åˆ"
            ])

        motion_analysis_lines.extend([
            "",
            "ğŸ”§ æŠ€æœ¯å‚æ•°:",
            "- éšæœºåŠ¨ä½œç”Ÿæˆ: å¯ç”¨" if motion_mode != "æ— åŠ¨ä½œå¢å¼º" else "- é»˜è®¤å¤„ç†æ¨¡å¼: å¯ç”¨",
            "- åŠ¨ä½œç±»å‹: çº¯éšæœºåˆ‡æ¢",
            "- æ—¶é—´æ§åˆ¶: æ¦‚ç‡åˆ†å¸ƒ",
            f"- å¼ºåº¦å€æ•°: {motion_intensity}x",
            "",
            "âš ï¸  æ³¨æ„: éšæœºåŠ¨ä½œæ¨¡å¼ï¼Œæ¯æ¬¡ç”Ÿæˆçš„æ—¶é—´çº¿éƒ½ä¸åŒ"
        ])

        logger.info(f"éšæœºåŠ¨ä½œæ§åˆ¶è®¾ç½®: æ¨¡å¼={motion_mode}, å¼ºåº¦={motion_intensity}")
        return "\n".join(motion_analysis_lines)


if __name__ == "__main__":
    processor = TTSDigitalHumanProcessor()

    # ç¦ç”¨é˜Ÿåˆ—åŠŸèƒ½ä¿®å¤stream.tsé”™è¯¯
    import os
    os.environ["GRADIO_SERVER_NAME"] = "0.0.0.0"

    with gr.Blocks(title="æ•°å­—äººè§†é¢‘ç”Ÿæˆç³»ç»Ÿ/Digital Human Video Generator") as demo:
        gr.Markdown("## ğŸ¬ æ•°å­—äººè§†é¢‘ç”Ÿæˆç³»ç»Ÿ/Digital Human Video Generator")
        gr.Markdown("æ”¯æŒTTSè¯­éŸ³åˆæˆæˆ–éŸ³é¢‘æ–‡ä»¶ä¸Šä¼ ï¼Œé…åˆéšæœºåŠ¨ä½œæ§åˆ¶ç”Ÿæˆæ•°å­—äººè§†é¢‘ã€‚/Support TTS speech synthesis or audio file upload, combined with random motion control to generate digital human videos.")

        with gr.Row():
            with gr.Column():
                # éŸ³é¢‘è¾“å…¥æ–¹å¼é€‰æ‹©
                with gr.Accordion("ğŸµ éŸ³é¢‘è¾“å…¥æ–¹å¼", open=True):
                    audio_input_mode = gr.Radio(
                        choices=[
                            ("TTSè¯­éŸ³åˆæˆ", "tts"),
                            ("ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶", "upload")
                        ],
                        value="tts",
                        label="é€‰æ‹©éŸ³é¢‘è¾“å…¥æ–¹å¼",
                        info="é€‰æ‹©ä½¿ç”¨TTSç”Ÿæˆè¯­éŸ³æˆ–ç›´æ¥ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"
                    )

                # TTSé…ç½®
                with gr.Accordion("ğŸ¤ TTSè¯­éŸ³åˆæˆé…ç½®", open=True):
                    with gr.Group() as tts_group:
                        api_key_input = gr.Textbox(
                            label="Minimax API Key",
                            placeholder="è¯·è¾“å…¥æ‚¨çš„Minimax API Key",
                            type="password"
                        )

                        model_input = gr.Textbox(
                            value="speech-2.6-hd",
                            label="TTSæ¨¡å‹",
                            placeholder="è¾“å…¥TTSæ¨¡å‹åç§°ï¼Œå¦‚ï¼šspeech-2.6-hd",
                            info="æ”¯æŒçš„æ¨¡å‹ï¼šspeech-01, speech-01-hd, speech-02, speech-02-hd, speech-2.6-hdç­‰"
                        )

                        voice_input = gr.Textbox(
                            value="female-shaonv",
                            label="å£°éŸ³ID",
                            placeholder="è¾“å…¥å£°éŸ³IDï¼Œå¦‚ï¼šfemale-shaonv",
                            info="å¸¸ç”¨å£°éŸ³ï¼šmale-qn-qingse, female-shaonv, female-yujie, male-qn-jingyingç­‰"
                        )

                        text_input = gr.Textbox(
                            label="è¾“å…¥æ–‡æœ¬",
                            placeholder="è¯·è¾“å…¥è¦åˆæˆè¯­éŸ³çš„æ–‡æœ¬å†…å®¹...",
                            lines=4,
                            max_lines=8
                        )

                # éŸ³é¢‘æ–‡ä»¶ä¸Šä¼ 
                with gr.Accordion("ğŸ“ éŸ³é¢‘æ–‡ä»¶ä¸Šä¼ ", open=False):
                    with gr.Group() as audio_group:
                        audio_file_input = gr.File(
                            label="ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (æ”¯æŒæ ¼å¼ï¼šWAV, MP3, M4A, FLAC, OGG)",
                            file_types=[".wav", ".mp3", ".m4a", ".flac", ".ogg"]
                        )

                # è§†é¢‘é…ç½®
                video_input = gr.File(label="ä¸Šä¼ è§†é¢‘æ–‡ä»¶/Upload video file")

                # åŠ¨ä½œæ§åˆ¶é€‰é¡¹
                with gr.Accordion("ğŸ­ éšæœºåŠ¨ä½œæ§åˆ¶é€‰é¡¹ (å®éªŒæ€§)", open=False):
                    motion_mode = gr.Dropdown(
                        choices=[
                            "æ— åŠ¨ä½œå¢å¼º",
                            "è½»å¾®ç‚¹å¤´",
                            "æ˜æ˜¾ç‚¹å¤´",
                            "æ€è€ƒæ­ªå¤´",
                            "éšæœºæ··åˆ",
                            "è‡ªå®šä¹‰é…ç½®"
                        ],
                        value="è½»å¾®ç‚¹å¤´",
                        label="éšæœºåŠ¨ä½œç±»å‹",
                        info="é€‰æ‹©æ•°å­—äººçš„éšæœºåŠ¨ä½œæ¨¡å¼"
                    )
                    motion_intensity = gr.Slider(
                        minimum=0.1,
                        maximum=2.0,
                        value=1.0,
                        step=0.1,
                        label="åŠ¨ä½œå¼ºåº¦",
                        info="æ§åˆ¶åŠ¨ä½œçš„å¼ºçƒˆç¨‹åº¦"
                    )

                    # åŠ¨ä½œæƒé‡é…ç½®
                    with gr.Row():
                        still_weight = gr.Slider(
                            minimum=0.0,
                            maximum=1.0,
                            value=0.5,
                            step=0.05,
                            label="é™æ­¢æƒé‡",
                            info="é™æ­¢çŠ¶æ€çš„æ¦‚ç‡æƒé‡"
                        )
                        nod_weight = gr.Slider(
                            minimum=0.0,
                            maximum=1.0,
                            value=0.3,
                            step=0.05,
                            label="ç‚¹å¤´æƒé‡",
                            info="ç‚¹å¤´åŠ¨ä½œçš„æ¦‚ç‡æƒé‡"
                        )
                        tilt_weight = gr.Slider(
                            minimum=0.0,
                            maximum=1.0,
                            value=0.2,
                            step=0.05,
                            label="å€¾æ–œæƒé‡",
                            info="å¤´éƒ¨å€¾æ–œçš„æ¦‚ç‡æƒé‡"
                        )

                    # åŠ¨ä½œæ—¶é—´é…ç½®
                    with gr.Row():
                        interval_min = gr.Number(
                            value=2.0,
                            minimum=0.5,
                            maximum=10.0,
                            step=0.5,
                            label="æœ€å°é—´éš”æ—¶é—´(ç§’)",
                            info="åŠ¨ä½œåˆ‡æ¢çš„æœ€å°æ—¶é—´é—´éš”"
                        )
                        interval_max = gr.Number(
                            value=5.0,
                            minimum=1.0,
                            maximum=15.0,
                            step=0.5,
                            label="æœ€å¤§é—´éš”æ—¶é—´(ç§’)",
                            info="åŠ¨ä½œåˆ‡æ¢çš„æœ€å¤§æ—¶é—´é—´éš”"
                        )

                    # åŠ¨ä½œå¹…åº¦é…ç½®
                    with gr.Row():
                        nod_amplitude_min = gr.Number(
                            value=3.0,
                            minimum=1.0,
                            maximum=20.0,
                            step=0.5,
                            label="ç‚¹å¤´æœ€å°å¹…åº¦(åº¦)",
                            info="ç‚¹å¤´åŠ¨ä½œçš„æœ€å°è§’åº¦"
                        )
                        nod_amplitude_max = gr.Number(
                            value=8.0,
                            minimum=2.0,
                            maximum=25.0,
                            step=0.5,
                            label="ç‚¹å¤´æœ€å¤§å¹…åº¦(åº¦)",
                            info="ç‚¹å¤´åŠ¨ä½œçš„æœ€å¤§è§’åº¦"
                        )

                    with gr.Row():
                        tilt_amplitude_min = gr.Number(
                            value=3.0,
                            minimum=1.0,
                            maximum=20.0,
                            step=0.5,
                            label="å€¾æ–œæœ€å°å¹…åº¦(åº¦)",
                            info="å¤´éƒ¨å€¾æ–œçš„æœ€å°è§’åº¦"
                        )
                        tilt_amplitude_max = gr.Number(
                            value=8.0,
                            minimum=2.0,
                            maximum=25.0,
                            step=0.5,
                            label="å€¾æ–œæœ€å¤§å¹…åº¦(åº¦)",
                            info="å¤´éƒ¨å€¾æ–œçš„æœ€å¤§è§’åº¦"
                        )

                submit_btn = gr.Button("ğŸ¬ ç”ŸæˆTTSæ•°å­—äººè§†é¢‘", variant="primary", size="lg")

            with gr.Column():
                video_output = gr.Video(label="ç”Ÿæˆçš„æ•°å­—äººè§†é¢‘/Generated Digital Human Video")

                # æ˜¾ç¤ºåˆ†æç»“æœ
                with gr.Accordion("ğŸ“Š éŸ³é¢‘å¤„ç†æŠ¥å‘Š", open=True):
                    audio_analysis = gr.Textbox(
                        label="éŸ³é¢‘å¤„ç†è¯¦æƒ…",
                        placeholder="éŸ³é¢‘å¤„ç†å®Œæˆåå°†æ˜¾ç¤ºåˆ†ææŠ¥å‘Š...",
                        lines=8
                    )

                with gr.Accordion("ğŸ“Š åŠ¨ä½œåˆ†ææŠ¥å‘Š", open=False):
                    motion_analysis = gr.Textbox(
                        label="åŠ¨ä½œæ‰§è¡Œæ—¥å¿—",
                        placeholder="åŠ¨ä½œå¤„ç†å®Œæˆåå°†æ˜¾ç¤ºåˆ†æç»“æœ...",
                        lines=5
                    )

        # æ·»åŠ ä½¿ç”¨è¯´æ˜
        with gr.Accordion("ğŸ“– ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("""
            ### ğŸ”§ ä½¿ç”¨æ­¥éª¤ï¼š
            1. **é€‰æ‹©éŸ³é¢‘è¾“å…¥æ–¹å¼**ï¼šé€‰æ‹©TTSè¯­éŸ³åˆæˆæˆ–ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
            2. **é…ç½®éŸ³é¢‘å‚æ•°**ï¼š
               - TTSæ¨¡å¼ï¼šè¾“å…¥API Keyã€æ–‡æœ¬å†…å®¹ã€é€‰æ‹©æ¨¡å‹å’Œå£°éŸ³
               - ä¸Šä¼ æ¨¡å¼ï¼šé€‰æ‹©éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒWAVã€MP3ã€M4Aã€FLACã€OGGæ ¼å¼ï¼‰
            3. **ä¸Šä¼ è§†é¢‘æ–‡ä»¶**ï¼šé€‰æ‹©ä¸€ä¸ªåŒ…å«äººè„¸çš„è§†é¢‘æ–‡ä»¶
            4. **é…ç½®éšæœºåŠ¨ä½œæ§åˆ¶**ï¼šæ ¹æ®éœ€è¦é€‰æ‹©éšæœºåŠ¨ä½œæ¨¡å¼å’Œå¼ºåº¦
            5. **ç”Ÿæˆè§†é¢‘**ï¼šç‚¹å‡»ç”ŸæˆæŒ‰é’®ï¼Œç­‰å¾…å¤„ç†å®Œæˆ

            ### ğŸµ å¸¸ç”¨å£°éŸ³IDï¼š
            - **ç”·å£°**ï¼šmale-qn-qingse (é’æ¶©é’å¹´), male-qn-jingying (ç²¾è‹±), male-qn-badao (éœ¸é“), male-qn-daxuesheng (å¤§å­¦ç”Ÿ)
            - **å¥³å£°**ï¼šfemale-shaonv (å°‘å¥³), female-yujie (å¾¡å§), female-chengshu (æˆç†Ÿ), female-tianmei (ç”œç¾)
            - **æ›´å¤š**ï¼šfemale-qn-qingse, female-qn-jingying, female-qn-badao, female-qn-daxueshengç­‰

            ### âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
            - éœ€è¦æœ‰æ•ˆçš„Minimax API Key
            - æ–‡æœ¬é•¿åº¦å»ºè®®æ§åˆ¶åœ¨1000å­—ç¬¦ä»¥å†…
            - è§†é¢‘æ–‡ä»¶éœ€è¦åŒ…å«æ¸…æ™°çš„äººè„¸
            - APIè°ƒç”¨ä¼šæ¶ˆè€—tokensï¼Œè¯·åˆç†ä½¿ç”¨
            """)

        # ç®€åŒ–ç•Œé¢ï¼šæ˜¾ç¤ºè¯´æ˜ä¿¡æ¯è€Œä¸æ˜¯å¤æ‚çš„åˆ‡æ¢é€»è¾‘
        gr.Markdown("""
        ğŸ’¡ **ä½¿ç”¨è¯´æ˜**ï¼š
        - å¦‚æœé€‰æ‹© "TTSè¯­éŸ³åˆæˆ"ï¼Œè¯·å¡«å†™ä¸Šæ–¹TTSé…ç½®å¹¶è¾“å…¥æ–‡æœ¬
        - å¦‚æœé€‰æ‹© "ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"ï¼Œè¯·åœ¨ä¸‹æ–¹ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
        - ä¸¤ç§æ¨¡å¼æ ¹æ®æ‚¨åœ¨ä¸Šé¢çš„é€‰æ‹©è‡ªåŠ¨ç”Ÿæ•ˆï¼Œæ— éœ€é¢å¤–æ“ä½œ
        """)

        # ç»‘å®šæäº¤äº‹ä»¶
        submit_btn.click(
            fn=processor.generate_digital_human_from_video,
            inputs=[
                audio_input_mode,
                api_key_input,
                voice_input,
                text_input,
                model_input,
                audio_file_input,
                video_input,
                motion_mode,
                motion_intensity,
                still_weight,
                nod_weight,
                tilt_weight,
                interval_min,
                interval_max,
                nod_amplitude_min,
                nod_amplitude_max,
                tilt_amplitude_min,
                tilt_amplitude_max
            ],
            outputs=[video_output, audio_analysis, motion_analysis],
            queue=False
        )

    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True
    )